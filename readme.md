Voici le contenu complet du `README.md` restructurÃ©, incluant la nouvelle section **FonctionnalitÃ©s Ã  venir** :

---

# ğŸï¸ Lakehouse Project

## ğŸŒŸ Objectif

Ce projet met en place un **Lakehouse** complet pour le traitement et lâ€™analyse de donnÃ©es Ã  grande Ã©chelle, en combinant :

- âš¡ **Apache Spark** : traitement distribuÃ©  
- ğŸ§Š **Apache Iceberg** : format de table transactionnel  
- ğŸ” **Trino** : moteur SQL interactif  
- ğŸ“Š **Apache Superset** : visualisation et reporting BI  
- â˜ï¸ **MinIO** : stockage S3 compatible  
- ğŸ³ **Docker & Docker Compose** : orchestration locale  
- ğŸŒ **Terraform** : provisionnement AWS  

ğŸ¯ Lâ€™objectif est de fournir un environnement **modulaire, scalable et reproductible** pour tester et dÃ©ployer des pipelines data.

---

## ğŸ—ï¸ Architecture

```text
+-------------------+       +-------------------+
| Jupyter Notebook  | <---> |      Spark        |
+-------------------+       +-------------------+
                                  |
                                  v
                          +-------------------+
                          |     Iceberg       |
                          +-------------------+
                                  |
                                  v
                          +-------------------+
                          |      Trino        |
                          +-------------------+
                                  |
                                  v
                          +-------------------+
                          |     Superset      |
                          +-------------------+

+-------------------+
|      MinIO        |
+-------------------+
```

---

## âš™ï¸ PrÃ©requis

- Docker & Docker Compose  
- Terraform â‰¥ 1.5  
- AWS CLI configurÃ©  
- Python â‰¥ 3.10  

---

## ğŸš€ Installation

### 1. DÃ©ploiement de lâ€™infrastructure AWS

```bash
terraform init
terraform apply
```

> ğŸ’¡ Si tu rencontres lâ€™erreur suivante :
```bash
InvalidKeyPair.Duplicate: The keypair already exists
```
ExÃ©cute :

```bash
aws ec2 delete-key-pair --key-name demo
terraform import aws_key_pair.default demo
```

---

### 2. Lancement des services Docker

```bash
docker-compose up -d
```

---

### 3. AccÃ¨s aux interfaces

- ğŸ““ **Jupyter Notebook** : http://localhost:8888  
- ğŸ“Š **Superset Dashboard** : http://localhost:8088  
  - **Username** : `admin`  
  - **Password** : `admin`  

---

## ğŸ“š Utilisation

### ğŸ”¬ Notebooks

- `notebooks/SparkSQL.ipynb` : requÃªtes SQL sur Iceberg  

### âš™ï¸ Configuration

- `trino/iceberg.properties` : config Trino  
- `infra/` : ressources Terraform  

### ğŸ”— Connexion Superset

- **SQLAlchemy URI** :

```bash
trino://trino@trino:8080/iceberg/
```

- **Ajout dans Superset** :
  1. Ouvre http://localhost:8088
  2. Va dans **Data â†’ Databases â†’ +**
  3. Colle lâ€™URI ci-dessus

---

## ğŸ”’ SÃ©curitÃ©

- Les **Security Groups AWS** exposent uniquement les ports nÃ©cessaires.  
- Les fichiers sensibles (`*.pem`, `*.tfvars`) sont exclus via `.gitignore`.

!Architecture rÃ©seau

---

## ğŸ“ˆ Avantages

- Environnement reproductible  
- Scalable avec Spark & Trino  
- Isolation via Docker  
- Compatible AWS & S3 local  

---

## ğŸ“ Commandes utiles

| ğŸ“¦ Composant        | ğŸ› ï¸ Commande                                      |
|---------------------|--------------------------------------------------|
| DÃ©marrer les services | `docker-compose up --build -d`                |
| ArrÃªter les services  | `docker-compose down`                         |
| Voir les conteneurs   | `docker ps`                                   |
| Logs en direct        | `docker-compose logs -f`                      |
| Rebuild complet       | `docker-compose up --build --force-recreate -d` |

---

## ğŸ› ï¸ FonctionnalitÃ©s Ã  venir

Voici les Ã©volutions prÃ©vues pour enrichir l'Ã©cosystÃ¨me Lakehouse :

- âš™ï¸ **Apache Flink + Debezium** : ingestion de donnÃ©es en temps rÃ©el via CDC (Change Data Capture)  
- ğŸ§  **OpenMetadata** : gouvernance des mÃ©tadonnÃ©es et data catalog centralisÃ©  
- ğŸ§¬ **DBT (Data Build Tool)** : gestion des transformations SQL et documentation des modÃ¨les  
- ğŸ”„ **Airbyte** : intÃ©gration automatisÃ©e des donnÃ©es brutes depuis diverses sources (APIs, bases de donnÃ©es, etc.)

ğŸ¯ Ces ajouts permettront d'Ã©tendre le projet vers un pipeline complet de donnÃ©es temps rÃ©el, gouvernÃ© et documentÃ©.

